import math

import numpy as np
import torch
from model.tgn_mulit import TGN
from sklearn.metrics import average_precision_score, roc_auc_score

from sklearn.metrics import precision_recall_fscore_support, roc_auc_score

def eval_node_classification(tgn, data, edge_idxs, batch_size, n_neighbors, num_classes):
    pred_prob = np.zeros((len(data.sources), num_classes))  # Initialize as 2D array (num_samples, num_classes)
    true_labels = np.zeros(len(data.sources))  # Store true labels
    num_instance = len(data.sources)
    num_batch = math.ceil(num_instance / batch_size)

    with torch.no_grad():
        tgn.eval()  # Ensure model is in evaluation mode
        for k in range(num_batch):
            s_idx = k * batch_size
            e_idx = min(num_instance, s_idx + batch_size)

            sources_batch = data.sources[s_idx:e_idx]
            destinations_batch = data.destinations[s_idx:e_idx]
            timestamps_batch = data.timestamps[s_idx:e_idx]
            edge_idxs_batch = edge_idxs[s_idx:e_idx]
            labels_batch = data.labels[s_idx:e_idx]

            # è®¡ç®—æºèŠ‚ç‚¹çš„åµŒå…¥
            source_embedding, destination_embedding, _ = tgn.compute_temporal_embeddings(
                sources_batch, destinations_batch, destinations_batch, timestamps_batch, edge_idxs_batch, n_neighbors
            )

            # ç›´æŽ¥ä½¿ç”¨æ¨¡åž‹çš„è¾“å‡ºï¼ˆè€Œä¸æ˜¯å¤–éƒ¨çš„è§£ç å™¨ï¼‰
            node_classification_logits = tgn.node_classification_decoder(source_embedding)

            # ä½¿ç”¨ softmax èŽ·å¾—ç±»åˆ«æ¦‚çŽ‡
            pred_prob_batch = torch.softmax(node_classification_logits, dim=-1)
            

            # å­˜å‚¨é¢„æµ‹ç»“æžœ
            pred_prob[s_idx:e_idx] = pred_prob_batch.cpu().numpy()
            true_labels[s_idx:e_idx] = labels_batch  # å­˜å‚¨çœŸå®žæ ‡ç­¾

    # è®¡ç®—æ¯ä¸ªç±»çš„ AUC å€¼ï¼ˆone-vs-rest ç­–ç•¥ï¼‰
    auc_roc = roc_auc_score(true_labels, pred_prob, average=None, multi_class='ovr')

    # è®¡ç®—æ¯ä¸ªç±»çš„ç²¾åº¦ã€å¬å›žçŽ‡å’Œ F1 åˆ†æ•°
    precision, recall, f1, support = precision_recall_fscore_support(
        true_labels, np.argmax(pred_prob, axis=1), average=None, zero_division=0)

    # è¿”å›žè¯„ä¼°ç»“æžœ
    return auc_roc, precision, recall, f1, support, pred_prob


def eval_node_classification_ddp(tgn, data, edge_idxs, batch_size, n_neighbors, num_classes):
    pred_prob = np.zeros((len(data.sources), num_classes))  # Initialize as 2D array (num_samples, num_classes)
    true_labels = np.zeros(len(data.sources))  # Store true labels
    num_instance = len(data.sources)
    num_batch = math.ceil(num_instance / batch_size)

    with torch.no_grad():
        tgn.eval()  # Ensure model is in evaluation mode
        for k in range(num_batch):
            s_idx = k * batch_size
            e_idx = min(num_instance, s_idx + batch_size)

            sources_batch = data.sources[s_idx:e_idx]
            destinations_batch = data.destinations[s_idx:e_idx]
            timestamps_batch = data.timestamps[s_idx:e_idx]
            edge_idxs_batch = edge_idxs[s_idx:e_idx]
            labels_batch = data.labels[s_idx:e_idx]

            # è®¡ç®—æºèŠ‚ç‚¹çš„åµŒå…¥
            source_embedding, destination_embedding, _ = tgn.module.compute_temporal_embeddings(
                sources_batch, destinations_batch, destinations_batch, timestamps_batch, edge_idxs_batch, n_neighbors
            )

            # ç›´æŽ¥ä½¿ç”¨æ¨¡åž‹çš„è¾“å‡ºï¼ˆè€Œä¸æ˜¯å¤–éƒ¨çš„è§£ç å™¨ï¼‰
            node_classification_logits = tgn.module.node_classification_decoder(source_embedding)

            # ä½¿ç”¨ softmax èŽ·å¾—ç±»åˆ«æ¦‚çŽ‡
            pred_prob_batch = torch.softmax(node_classification_logits, dim=-1)
            

            # å­˜å‚¨é¢„æµ‹ç»“æžœ
            pred_prob[s_idx:e_idx] = pred_prob_batch.cpu().numpy()
            true_labels[s_idx:e_idx] = labels_batch  # å­˜å‚¨çœŸå®žæ ‡ç­¾

    # è®¡ç®—æ¯ä¸ªç±»çš„ AUC å€¼ï¼ˆone-vs-rest ç­–ç•¥ï¼‰
    auc_roc = roc_auc_score(true_labels, pred_prob, average=None, multi_class='ovr')

    # è®¡ç®—æ¯ä¸ªç±»çš„ç²¾åº¦ã€å¬å›žçŽ‡å’Œ F1 åˆ†æ•°
    precision, recall, f1, support = precision_recall_fscore_support(
        true_labels, np.argmax(pred_prob, axis=1), average=None, zero_division=0)

    # è¿”å›žè¯„ä¼°ç»“æžœ
    return auc_roc, precision, recall, f1, support, pred_prob




from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_fscore_support

def eval_edge_prediction_add(model, negative_edge_sampler, data, n_neighbors, batch_size=200):
    assert negative_edge_sampler.seed is not None
    negative_edge_sampler.reset_random_state()

    val_ap, val_auc, val_precision, val_recall, val_f1, val_acc = [], [], [], [], [], []

    with torch.no_grad():
        model = model.eval()  # Ensure model is in evaluation mode
        TEST_BATCH_SIZE = batch_size
        num_test_instance = len(data.sources)
        num_test_batch = math.ceil(num_test_instance / TEST_BATCH_SIZE)

        for k in range(num_test_batch):
            s_idx = k * TEST_BATCH_SIZE
            e_idx = min(num_test_instance, s_idx + TEST_BATCH_SIZE)
            sources_batch = data.sources[s_idx:e_idx]
            destinations_batch = data.destinations[s_idx:e_idx]
            timestamps_batch = data.timestamps[s_idx:e_idx]
            edge_idxs_batch = data.edge_idxs[s_idx:e_idx]

            size = len(sources_batch)
            _, negative_samples = negative_edge_sampler.sample(size)

            # ç›´æŽ¥è®¡ç®—æ­£è´Ÿè¾¹çš„æ¦‚çŽ‡
            pos_prob, neg_prob = model.compute_edge_probabilities(
                sources_batch, destinations_batch,
                negative_samples, timestamps_batch,
                edge_idxs_batch, n_neighbors
            )

            # åˆå¹¶æ­£è´Ÿæ ·æœ¬çš„é¢„æµ‹åˆ†æ•°
            y_score = np.concatenate([pos_prob.cpu().numpy(), neg_prob.cpu().numpy()])
            y_true = np.concatenate([np.ones(size), np.zeros(size)])
            y_pred = (y_score >= 0.5).astype(int)  # äºŒåˆ†ç±»ï¼ˆé˜ˆå€¼ä¸º 0.5ï¼‰

            # è®¡ç®—è¯„ä»·æŒ‡æ ‡
            val_ap.append(average_precision_score(y_true, y_score))
            val_auc.append(roc_auc_score(y_true, y_score))

            # è®¡ç®—ç²¾åº¦ã€å¬å›žçŽ‡ã€F1åˆ†æ•°
            precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)
            val_precision.append(precision)
            val_recall.append(recall)
            val_f1.append(f1)

            # è®¡ç®—å‡†ç¡®çŽ‡
            accuracy = (y_pred == y_true).mean()
            val_acc.append(accuracy)

    # è¿”å›žå¹³å‡å€¼
    return np.mean(val_ap), np.mean(val_auc), np.mean(val_precision), np.mean(val_recall), np.mean(val_f1), np.mean(val_acc)

def eval_edge_prediction_add_ddp(model, negative_edge_sampler, data, n_neighbors, batch_size=200):
    assert negative_edge_sampler.seed is not None
    negative_edge_sampler.reset_random_state()

    val_ap, val_auc, val_precision, val_recall, val_f1, val_acc = [], [], [], [], [], []

    with torch.no_grad():
        model = model.eval()  # Ensure model is in evaluation mode
        TEST_BATCH_SIZE = batch_size
        num_test_instance = len(data.sources)
        num_test_batch = math.ceil(num_test_instance / TEST_BATCH_SIZE)

        for k in range(num_test_batch):
            s_idx = k * TEST_BATCH_SIZE
            e_idx = min(num_test_instance, s_idx + TEST_BATCH_SIZE)
            sources_batch = data.sources[s_idx:e_idx]
            destinations_batch = data.destinations[s_idx:e_idx]
            timestamps_batch = data.timestamps[s_idx:e_idx]
            edge_idxs_batch = data.edge_idxs[s_idx:e_idx]

            size = len(sources_batch)
            _, negative_samples = negative_edge_sampler.sample(size)

            # ç›´æŽ¥è®¡ç®—æ­£è´Ÿè¾¹çš„æ¦‚çŽ‡
            pos_prob, neg_prob = model.module.compute_edge_probabilities(
                sources_batch, destinations_batch,
                negative_samples, timestamps_batch,
                edge_idxs_batch, n_neighbors
            )

            # åˆå¹¶æ­£è´Ÿæ ·æœ¬çš„é¢„æµ‹åˆ†æ•°
            y_score = np.concatenate([pos_prob.cpu().numpy(), neg_prob.cpu().numpy()])
            y_true = np.concatenate([np.ones(size), np.zeros(size)])
            y_pred = (y_score >= 0.5).astype(int)  # äºŒåˆ†ç±»ï¼ˆé˜ˆå€¼ä¸º 0.5ï¼‰

            # è®¡ç®—è¯„ä»·æŒ‡æ ‡
            val_ap.append(average_precision_score(y_true, y_score))
            val_auc.append(roc_auc_score(y_true, y_score))

            # è®¡ç®—ç²¾åº¦ã€å¬å›žçŽ‡ã€F1åˆ†æ•°
            precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)
            val_precision.append(precision)
            val_recall.append(recall)
            val_f1.append(f1)

            # è®¡ç®—å‡†ç¡®çŽ‡
            accuracy = (y_pred == y_true).mean()
            val_acc.append(accuracy)

    # è¿”å›žå¹³å‡å€¼
    return np.mean(val_ap), np.mean(val_auc), np.mean(val_precision), np.mean(val_recall), np.mean(val_f1), np.mean(val_acc)

# import torch
# import numpy as np
# from sklearn.metrics import roc_auc_score, precision_recall_fscore_support
# import torch.distributed as dist

# def eval_node_classification_ddp_new(tgn, data, edge_idxs, batch_size, n_neighbors, num_classes):
#     device = next(tgn.parameters()).device  # èŽ·å–å½“å‰æ¨¡åž‹æ‰€åœ¨è®¾å¤‡
#     local_pred_prob = []
#     local_true_labels = []

#     num_instance = len(data.sources)
#     num_batch = math.ceil(num_instance / batch_size)

#     with torch.no_grad():
#         tgn.eval()  # ç¡®ä¿æ¨¡åž‹åœ¨ eval æ¨¡å¼

#         for k in range(num_batch):
#             s_idx = k * batch_size
#             e_idx = min(num_instance, s_idx + batch_size)

#             sources_batch = data.sources[s_idx:e_idx]
#             destinations_batch = data.destinations[s_idx:e_idx]
#             timestamps_batch = data.timestamps[s_idx:e_idx]
#             edge_idxs_batch = edge_idxs[s_idx:e_idx]
#             labels_batch = data.labels[s_idx:e_idx]

#             # èŽ·å–åµŒå…¥
#             source_embedding, destination_embedding, _ = tgn.module.compute_temporal_embeddings(
#                 sources_batch, destinations_batch, destinations_batch,
#                 timestamps_batch, edge_idxs_batch, n_neighbors
#             )

#             logits = tgn.module.node_classification_decoder(source_embedding)
#             prob = torch.softmax(logits, dim=-1)

#             local_pred_prob.append(prob.cpu())
#             local_true_labels.append(torch.tensor(labels_batch, dtype=torch.long))

#     # æ‹¼æŽ¥å½“å‰ rank çš„æ‰€æœ‰ batch
#     local_pred_prob = torch.cat(local_pred_prob, dim=0).to(device)
#     local_true_labels = torch.cat(local_true_labels, dim=0).to(device)

#     # ä½¿ç”¨ all_gather æ”¶é›†æ‰€æœ‰ rank çš„ç»“æžœ
#     world_size = dist.get_world_size()
#     gathered_pred_prob = [torch.zeros_like(local_pred_prob) for _ in range(world_size)]
#     gathered_true_labels = [torch.zeros_like(local_true_labels) for _ in range(world_size)]

#     dist.all_gather(gathered_pred_prob, local_pred_prob)
#     dist.all_gather(gathered_true_labels, local_true_labels)

#     # åˆå¹¶æ‰€æœ‰ GPU çš„ç»“æžœ
#     all_pred_prob = torch.cat(gathered_pred_prob, dim=0).cpu().numpy()
#     all_true_labels = torch.cat(gathered_true_labels, dim=0).cpu().numpy()

#     # è®¡ç®— AUCã€Precisionã€Recallã€F1ã€Support
#     auc_roc = roc_auc_score(all_true_labels, all_pred_prob, average=None, multi_class='ovr')
#     precision, recall, f1, support = precision_recall_fscore_support(
#         all_true_labels, np.argmax(all_pred_prob, axis=1), average=None, zero_division=0
#     )

#     return auc_roc, precision, recall, f1, support, all_pred_prob

# import torch
# import numpy as np
# from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_fscore_support
# import torch.distributed as dist

# def eval_edge_prediction_add_ddp_new(model, negative_edge_sampler, data, n_neighbors, batch_size=200):
#     assert negative_edge_sampler.seed is not None
#     negative_edge_sampler.reset_random_state()

#     device = next(model.parameters()).device

#     local_y_score = []
#     local_y_true = []

#     with torch.no_grad():
#         model.eval()
#         num_test_instance = len(data.sources)
#         num_test_batch = math.ceil(num_test_instance / batch_size)

#         for k in range(num_test_batch):
#             s_idx = k * batch_size
#             e_idx = min(num_test_instance, s_idx + batch_size)

#             sources_batch = data.sources[s_idx:e_idx]
#             destinations_batch = data.destinations[s_idx:e_idx]
#             timestamps_batch = data.timestamps[s_idx:e_idx]
#             edge_idxs_batch = data.edge_idxs[s_idx:e_idx]

#             size = len(sources_batch)
#             _, negative_samples = negative_edge_sampler.sample(size)

#             pos_prob, neg_prob = model.module.compute_edge_probabilities(
#                 sources_batch, destinations_batch, negative_samples,
#                 timestamps_batch, edge_idxs_batch, n_neighbors
#             )

#             pos_prob = pos_prob.squeeze().cpu()
#             neg_prob = neg_prob.squeeze().cpu()

#             y_score = np.concatenate([pos_prob.numpy(), neg_prob.numpy()])
#             y_true = np.concatenate([np.ones(size), np.zeros(size)])

#             local_y_score.append(torch.tensor(y_score, dtype=torch.float, device=device))
#             local_y_true.append(torch.tensor(y_true, dtype=torch.float, device=device))

#     # åˆå¹¶å½“å‰è¿›ç¨‹æ‰€æœ‰æ‰¹æ¬¡ç»“æžœ
#     local_y_score = torch.cat(local_y_score)
#     local_y_true = torch.cat(local_y_true)

#     # æ‰€æœ‰è¿›ç¨‹ all_gather æ±‡æ€»
#     world_size = dist.get_world_size()
#     gathered_score = [torch.zeros_like(local_y_score) for _ in range(world_size)]
#     gathered_true = [torch.zeros_like(local_y_true) for _ in range(world_size)]

#     dist.all_gather(gathered_score, local_y_score)
#     dist.all_gather(gathered_true, local_y_true)

#     # æ‹¼æŽ¥ä¸ºå®Œæ•´ç»“æžœ
#     y_score = torch.cat(gathered_score).cpu().numpy()
#     y_true = torch.cat(gathered_true).cpu().numpy()
#     y_pred = (y_score >= 0.5).astype(int)

#     # æŒ‡æ ‡è®¡ç®—
#     val_ap = average_precision_score(y_true, y_score)
#     val_auc = roc_auc_score(y_true, y_score)
#     precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)
#     accuracy = (y_pred == y_true).mean()

#     return val_ap, val_auc, precision, recall, f1, accuracy

import torch
import numpy as np
from sklearn.metrics import roc_auc_score, precision_recall_fscore_support
import torch.distributed as dist
import math

def eval_node_classification_ddp_new(tgn, data, edge_idxs, batch_size, n_neighbors, num_classes):
    device = next(tgn.parameters()).device
    local_pred_prob = []
    local_true_labels = []

    num_instance = len(data.sources)
    num_batch = math.ceil(num_instance / batch_size)

    with torch.no_grad():
        tgn.eval()

        for k in range(num_batch):
            s_idx = k * batch_size
            e_idx = min(num_instance, s_idx + batch_size)

            sources_batch = data.sources[s_idx:e_idx]
            destinations_batch = data.destinations[s_idx:e_idx]
            timestamps_batch = data.timestamps[s_idx:e_idx]
            edge_idxs_batch = edge_idxs[s_idx:e_idx]
            labels_batch = data.labels[s_idx:e_idx]

            source_embedding, destination_embedding, _ = tgn.module.compute_temporal_embeddings(
                sources_batch, destinations_batch, destinations_batch,
                timestamps_batch, edge_idxs_batch, n_neighbors
            )

            logits = tgn.module.node_classification_decoder(source_embedding)
            prob = torch.softmax(logits, dim=-1)

            local_pred_prob.append(prob.cpu())
            local_true_labels.append(torch.tensor(labels_batch, dtype=torch.long))

    # concat å½“å‰è¿›ç¨‹
    local_pred_prob = torch.cat(local_pred_prob, dim=0).cpu().numpy()
    local_true_labels = torch.cat(local_true_labels, dim=0).cpu().numpy()

    # all_gather_object æ›´å…¼å®¹å˜é•¿ batch
    gathered_probs, gathered_labels = [None] * dist.get_world_size(), [None] * dist.get_world_size()
    dist.all_gather_object(gathered_probs, local_pred_prob.tolist())
    dist.all_gather_object(gathered_labels, local_true_labels.tolist())

    # æ‹¼æŽ¥å›žå®Œæ•´æ ·æœ¬
    all_pred_prob = np.concatenate([np.array(p) for p in gathered_probs], axis=0)
    all_true_labels = np.concatenate([np.array(l) for l in gathered_labels], axis=0)

    # é˜²æ­¢æŸäº›ç±»ç¼ºå¤±æ—¶ AUC æŠ¥é”™
    try:
        auc_roc = roc_auc_score(all_true_labels, all_pred_prob, average=None, multi_class='ovr')
        weighted_auc_roc = roc_auc_score(all_true_labels, all_pred_prob, average='weighted', multi_class='ovr')

    except ValueError as e:
        print(f"[Warning] AUC error: {e}")
        auc_roc = np.full(num_classes, np.nan)

    precision, recall, f1, support = precision_recall_fscore_support(
        all_true_labels, np.argmax(all_pred_prob, axis=1), average=None, zero_division=0
    )
    # Optionally, calculate weighted average metrics (useful in class imbalance cases)
    weighted_precision, weighted_recall, weighted_f1, weighted_support = precision_recall_fscore_support(
        all_true_labels, np.argmax(all_pred_prob, axis=1), average='weighted', zero_division=0
    )

    return auc_roc, precision, recall, f1, support, all_pred_prob,all_true_labels

import torch
import numpy as np
from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_fscore_support
import torch.distributed as dist
import math

def eval_edge_prediction_add_ddp_new(model, negative_edge_sampler, data, n_neighbors, batch_size=200):
    assert negative_edge_sampler.seed is not None
    negative_edge_sampler.reset_random_state()

    device = next(model.parameters()).device
    local_scores = []
    local_labels = []

    with torch.no_grad():
        model.eval()
        num_instance = len(data.sources)
        num_batch = math.ceil(num_instance / batch_size)

        for k in range(num_batch):
            s_idx = k * batch_size
            e_idx = min(num_instance, s_idx + batch_size)

            sources_batch = data.sources[s_idx:e_idx]
            destinations_batch = data.destinations[s_idx:e_idx]
            timestamps_batch = data.timestamps[s_idx:e_idx]
            edge_idxs_batch = data.edge_idxs[s_idx:e_idx]

            size = len(sources_batch)
            _, negatives_batch = negative_edge_sampler.sample(size)

            pos_prob, neg_prob = model.module.compute_edge_probabilities(
                sources_batch, destinations_batch, negatives_batch,
                timestamps_batch, edge_idxs_batch, n_neighbors
            )

            pos_prob = pos_prob.squeeze().cpu().numpy()
            neg_prob = neg_prob.squeeze().cpu().numpy()

            y_score = np.concatenate([pos_prob, neg_prob])
            y_true = np.concatenate([np.ones_like(pos_prob), np.zeros_like(neg_prob)])

            local_scores.append(torch.tensor(y_score, dtype=torch.float, device=device))
            local_labels.append(torch.tensor(y_true, dtype=torch.float, device=device))

    # concat å½“å‰è¿›ç¨‹
    local_scores = torch.cat(local_scores)
    local_labels = torch.cat(local_labels)

    # gather
    world_size = dist.get_world_size()
    gathered_scores = [None for _ in range(world_size)]
    gathered_labels = [None for _ in range(world_size)]
    dist.all_gather_object(gathered_scores, local_scores.tolist())
    dist.all_gather_object(gathered_labels, local_labels.tolist())

    y_score = np.concatenate([np.array(x) for x in gathered_scores])
    y_true = np.concatenate([np.array(x) for x in gathered_labels])
    y_pred = (y_score >= 0.5).astype(int)

    val_ap = average_precision_score(y_true, y_score)
    val_auc = roc_auc_score(y_true, y_score)
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average='binary', zero_division=0
    )
    acc = (y_pred == y_true).mean()

    return val_ap, val_auc, precision, recall, f1, acc

import torch
import numpy as np
from sklearn.metrics import roc_auc_score, precision_recall_fscore_support
import torch.distributed as dist
import math

def eval_node_classification_ddp_new1(tgn, data, edge_idxs, batch_size, n_neighbors, num_classes):
    device = next(tgn.parameters()).device
    local_pred_prob = []
    local_true_labels = []

    num_instance = len(data.sources)
    num_batch = math.ceil(num_instance / batch_size)

    with torch.no_grad():
        tgn.eval()

        for k in range(num_batch):
            s_idx = k * batch_size
            e_idx = min(num_instance, s_idx + batch_size)

            sources_batch = data.sources[s_idx:e_idx]
            destinations_batch = data.destinations[s_idx:e_idx]
            timestamps_batch = data.timestamps[s_idx:e_idx]
            edge_idxs_batch = edge_idxs[s_idx:e_idx]
            labels_batch = data.labels[s_idx:e_idx]

            # Get node embeddings
            source_embedding, destination_embedding, _ = tgn.module.compute_temporal_embeddings(
                sources_batch, destinations_batch, destinations_batch,
                timestamps_batch, edge_idxs_batch, n_neighbors
            )

            # Compute logits and probabilities
            logits = tgn.module.node_classification_decoder(source_embedding)
            prob = torch.softmax(logits, dim=-1)

            # Store predictions and true labels
            local_pred_prob.append(prob.cpu())
            local_true_labels.append(torch.tensor(labels_batch, dtype=torch.long))

    # Concatenate predictions and labels across all processes
    local_pred_prob = torch.cat(local_pred_prob, dim=0).cpu().numpy()
    local_true_labels = torch.cat(local_true_labels, dim=0).cpu().numpy()

    # Gather predictions and labels across all processes
    gathered_probs, gathered_labels = [None] * dist.get_world_size(), [None] * dist.get_world_size()
    dist.all_gather_object(gathered_probs, local_pred_prob.tolist())
    dist.all_gather_object(gathered_labels, local_true_labels.tolist())

    # Concatenate to form the complete dataset
    all_pred_prob = np.concatenate([np.array(p) for p in gathered_probs], axis=0)
    all_true_labels = np.concatenate([np.array(l) for l in gathered_labels], axis=0)

    # Compute weighted AUC
    try:
        auc_roc = roc_auc_score(all_true_labels, all_pred_prob, average='weighted', multi_class='ovr')
    except ValueError as e:
        print(f"[Warning] AUC error: {e}")
        auc_roc = np.full(num_classes, np.nan)

    # Compute Precision, Recall, F1 for each class
    precision, recall, f1, support = precision_recall_fscore_support(
        all_true_labels, np.argmax(all_pred_prob, axis=1), average=None, zero_division=0
    )

    # Compute weighted average F1
    f1_weighted = np.average(f1, weights=support)

    # Compute macro and micro average metrics
    precision_macro, recall_macro, f1_macro, support_macro = precision_recall_fscore_support(
        all_true_labels, np.argmax(all_pred_prob, axis=1), average='macro', zero_division=0
    )

    precision_micro, recall_micro, f1_micro, support_micro = precision_recall_fscore_support(
        all_true_labels, np.argmax(all_pred_prob, axis=1), average='micro', zero_division=0
    )

    return auc_roc, precision, recall, f1, support, f1_weighted, precision_macro, recall_macro, f1_macro, support_macro, precision_micro, recall_micro, f1_micro, support_micro, all_pred_prob


import torch
import numpy as np
from sklearn.metrics import average_precision_score, roc_auc_score, precision_recall_fscore_support
import torch.distributed as dist
import math

def eval_edge_prediction_add_ddp_new1(model, negative_edge_sampler, data, n_neighbors, batch_size=200):
    assert negative_edge_sampler.seed is not None
    negative_edge_sampler.reset_random_state()

    device = next(model.parameters()).device
    local_scores = []
    local_labels = []

    with torch.no_grad():
        model.eval()
        num_instance = len(data.sources)
        num_batch = math.ceil(num_instance / batch_size)

        for k in range(num_batch):
            s_idx = k * batch_size
            e_idx = min(num_instance, s_idx + batch_size)

            sources_batch = data.sources[s_idx:e_idx]
            destinations_batch = data.destinations[s_idx:e_idx]
            timestamps_batch = data.timestamps[s_idx:e_idx]
            edge_idxs_batch = data.edge_idxs[s_idx:e_idx]

            size = len(sources_batch)
            _, negatives_batch = negative_edge_sampler.sample(size)

            # è®¡ç®—è¿žæŽ¥é¢„æµ‹çš„æ­£è´Ÿæ ·æœ¬æ¦‚çŽ‡
            pos_prob, neg_prob = model.module.compute_edge_probabilities(
                sources_batch, destinations_batch, negatives_batch,
                timestamps_batch, edge_idxs_batch, n_neighbors
            )

            pos_prob = pos_prob.squeeze().cpu().numpy()
            neg_prob = neg_prob.squeeze().cpu().numpy()

            y_score = np.concatenate([pos_prob, neg_prob])
            y_true = np.concatenate([np.ones_like(pos_prob), np.zeros_like(neg_prob)])

            # è®°å½•æ¯ä¸ªè¿›ç¨‹çš„ scores å’Œ labels
            local_scores.append(torch.tensor(y_score, dtype=torch.float, device=device))
            local_labels.append(torch.tensor(y_true, dtype=torch.float, device=device))

    # åˆå¹¶æ‰€æœ‰è¿›ç¨‹çš„ç»“æžœ
    local_scores = torch.cat(local_scores)
    local_labels = torch.cat(local_labels)

    # æ”¶é›†æ‰€æœ‰è¿›ç¨‹çš„æ•°æ®
    world_size = dist.get_world_size()
    gathered_scores = [None for _ in range(world_size)]
    gathered_labels = [None for _ in range(world_size)]
    dist.all_gather_object(gathered_scores, local_scores.tolist())
    dist.all_gather_object(gathered_labels, local_labels.tolist())

    # å°†æ‰€æœ‰è¿›ç¨‹çš„ç»“æžœæ‹¼æŽ¥æˆå®Œæ•´çš„é¢„æµ‹æ•°æ®
    y_score = np.concatenate([np.array(x) for x in gathered_scores])
    y_true = np.concatenate([np.array(x) for x in gathered_labels])
    
    # å°†é¢„æµ‹åˆ†æ•°è½¬ä¸ºé¢„æµ‹æ ‡ç­¾
    y_pred = (y_score >= 0.5).astype(int)

    # è®¡ç®— AUC å’Œå…¶ä»–è¯„ä¼°æŒ‡æ ‡
    val_ap = average_precision_score(y_true, y_score)
    val_auc = roc_auc_score(y_true, y_score, average='weighted')  # åŠ æƒ AUC
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average='binary', zero_division=0
    )
    acc = (y_pred == y_true).mean()

    return val_ap, val_auc, precision, recall, f1, acc

from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score
import torch

# def eval_node_classification_ddp_label(tgn, val_data, edge_idxs, batch_size, num_neighbors, num_classes):
#     device = next(tgn.parameters()).device
#     tgn = tgn.eval()

#     all_preds = []
#     all_labels = []
    
#     # å‡è®¾ val_data.sources å’Œ val_data.destinations æ˜¯å®žé™…çš„æ•°æ®æ•°ç»„
#     num_instances = len(val_data.sources)  # ä½¿ç”¨ sourcesï¼ˆæˆ–ä»»ä½•å…¶ä»–ç›¸å…³å±žæ€§ï¼‰

#     for batch_idx in range(0, num_instances, batch_size):
#         # ä»Ž val_data å±žæ€§ä¸­èŽ·å–æ‰¹æ¬¡æ•°æ®
#         sources_batch = val_data.sources[batch_idx: batch_idx + batch_size]
#         destinations_batch = val_data.destinations[batch_idx: batch_idx + batch_size]
#         timestamps_batch = val_data.timestamps[batch_idx: batch_idx + batch_size]
#         edge_idxs_batch = edge_idxs[batch_idx: batch_idx + batch_size]
#         labels_batch = val_data.labels[batch_idx: batch_idx + batch_size]

#         sources_batch = torch.tensor(sources_batch).to(device)
#         destinations_batch = torch.tensor(destinations_batch).to(device)
#         timestamps_batch = torch.tensor(timestamps_batch).to(device)
#         edge_idxs_batch = torch.tensor(edge_idxs_batch).to(device)
#         labels_batch = torch.tensor(labels_batch).to(device)

#         with torch.no_grad():
#             _, _, node_classification_logits = tgn(sources_batch, destinations_batch, None, timestamps_batch, edge_idxs_batch)

#         # å¯¹äºŽå¤šæ ‡ç­¾åˆ†ç±»ï¼Œä½¿ç”¨ sigmoid å‡½æ•°
#         probs = torch.sigmoid(node_classification_logits)

#         # é€šè¿‡è®¾å®šé˜ˆå€¼ï¼ˆ0.5ï¼‰æ¥èŽ·å–é¢„æµ‹ç»“æžœï¼ˆ0 æˆ– 1ï¼‰
#         preds = (probs > 0.5).float()

#         all_preds.append(preds.cpu().numpy())
#         all_labels.append(labels_batch.cpu().numpy())

#     # å°†åˆ—è¡¨è½¬æ¢ä¸º numpy æ•°ç»„ï¼Œä¾¿äºŽåŽç»­è®¡ç®—æŒ‡æ ‡
#     all_preds = np.concatenate(all_preds, axis=0)
#     all_labels = np.concatenate(all_labels, axis=0)

#     # è®¡ç®—æ¯ä¸ªæ ‡ç­¾çš„ AUC
#     auc_scores = []
#     for i in range(num_classes):
#         auc = roc_auc_score(all_labels[:, i], all_preds[:, i])
#         auc_scores.append(auc)
    
#     # è®¡ç®—å¤šæ ‡ç­¾çš„ precisionã€recall å’Œ f1
#     precision = precision_score(all_labels, all_preds, average='macro')
#     recall = recall_score(all_labels, all_preds, average='macro')
#     f1 = f1_score(all_labels, all_preds, average='macro')

#     return auc_scores, precision, recall, f1

# def eval_node_classification_ddp_label(tgn, data, edge_idxs, batch_size, n_neighbors, num_classes):
#     device = next(tgn.parameters()).device
#     local_pred_prob = []
#     local_true_labels = []

#     num_instance = len(data.sources)
#     num_batch = math.ceil(num_instance / batch_size)

#     with torch.no_grad():
#         tgn.eval()

#         for k in range(num_batch):
#             s_idx = k * batch_size
#             e_idx = min(num_instance, s_idx + batch_size)

#             sources_batch = data.sources[s_idx:e_idx]
#             destinations_batch = data.destinations[s_idx:e_idx]
#             timestamps_batch = data.timestamps[s_idx:e_idx]
#             edge_idxs_batch = edge_idxs[s_idx:e_idx]
#             labels_batch = data.labels[s_idx:e_idx]

#             source_embedding, destination_embedding, _ = tgn.module.compute_temporal_embeddings(
#                 sources_batch, destinations_batch, destinations_batch,
#                 timestamps_batch, edge_idxs_batch, n_neighbors
#             )

#             logits = tgn.module.node_classification_decoder(source_embedding)
#             print('logits',logits.shape)
#             prob = torch.softmax(logits, dim=-1)
#             print('prob',prob.shape)
            


#     return 

from sklearn.metrics import (
    f1_score, precision_score, recall_score, roc_auc_score,
    average_precision_score, hamming_loss
)
import torch.distributed as dist
import numpy as np
import torch

# def eval_node_classification_ddp_label(tgn, data, edge_idxs, batch_size, n_neighbors, num_classes, threshold=0.5):
#     device = next(tgn.parameters()).device
#     local_pred_prob = []
#     local_true_labels = []

#     num_instance = len(data.sources)
#     num_batch = math.ceil(num_instance / batch_size)

#     with torch.no_grad():
#         tgn.eval()

#         for k in range(num_batch):
#             s_idx = k * batch_size
#             e_idx = min(num_instance, s_idx + batch_size)

#             # sources_batch = torch.from_numpy(data.sources[s_idx:e_idx]).to(device)
#             # destinations_batch = torch.from_numpy(data.destinations[s_idx:e_idx]).to(device)
#             # timestamps_batch = torch.from_numpy(data.timestamps[s_idx:e_idx]).to(device)
#             # edge_idxs_batch = torch.from_numpy(edge_idxs[s_idx:e_idx]).to(device)
#             # labels_batch = torch.from_numpy(data.labels[s_idx:e_idx]).to(device).float()

#             # _, _, logits = tgn.module(
#             #     sources_batch, destinations_batch, None, timestamps_batch, edge_idxs_batch
#             # )
#             sources_batch = data.sources[s_idx:e_idx]
#             destinations_batch = data.destinations[s_idx:e_idx]
#             timestamps_batch = data.timestamps[s_idx:e_idx]
#             edge_idxs_batch = edge_idxs[s_idx:e_idx]
#             # labels_batch = data.labels[s_idx:e_idx]
            
#             # âš ï¸ ä½¿ç”¨çœŸå®žæ ‡ç­¾æ—¶æ³¨é‡Šæ­¤è¡Œï¼Œä½¿ç”¨éšæœºæ ‡ç­¾æ—¶å¯ç”¨
#             labels_batch = np.random.randint(0, 2, size=(e_idx - s_idx, num_classes)).astype(int)


#             # Get node embeddings
#             source_embedding, destination_embedding, _ = tgn.module.compute_temporal_embeddings(
#                 sources_batch, destinations_batch, destinations_batch,
#                 timestamps_batch, edge_idxs_batch, n_neighbors
#             )
            
#             logits = tgn.module.node_classification_decoder(source_embedding)
#             probs = torch.sigmoid(logits).cpu().numpy()     # shape: (batch_size, num_classes)
#             labels = labels_batch             # shape: (batch_size, num_classes)

#             local_pred_prob.append(probs)
#             local_true_labels.append(labels)

#     # åˆå¹¶æ‰€æœ‰ batch
#     local_pred_prob = np.concatenate(local_pred_prob, axis=0)
#     local_true_labels = np.concatenate(local_true_labels, axis=0)

#     # ========= DDP å¤šè¿›ç¨‹åŒæ­¥ =========
#     world_size = dist.get_world_size()

#     local_pred_prob_tensor = torch.tensor(local_pred_prob, device=device)
#     local_true_labels_tensor = torch.tensor(local_true_labels, device=device)

#     gathered_preds = [torch.zeros_like(local_pred_prob_tensor) for _ in range(world_size)]
#     gathered_trues = [torch.zeros_like(local_true_labels_tensor) for _ in range(world_size)]

#     dist.all_gather(gathered_preds, local_pred_prob_tensor)
#     dist.all_gather(gathered_trues, local_true_labels_tensor)

#     all_pred_prob = torch.cat(gathered_preds, dim=0).cpu().numpy()
#     all_true_labels = torch.cat(gathered_trues, dim=0).cpu().numpy()

#     # ========= å¤šæ ‡ç­¾è¯„ä¼° =========
#     y_pred = (all_pred_prob >= threshold).astype(int)
#     y_true = all_true_labels
    
#     # print('y_true shape:', y_true.shape, 'dtype:', y_true.dtype)
#     # print('y_pred shape:', y_pred.shape, 'dtype:', y_pred.dtype)
#     # print('unique values in y_true:', np.unique(y_true))

    
#     # ðŸ‘‡ å¼ºåˆ¶è½¬æ¢ä¸ºæ•´æ•°ï¼ˆé¿å… sklearn è¯¯åˆ¤ä¸º multiclassï¼‰
#     y_pred = np.array(y_pred, dtype=int)
#     y_true = np.array(y_true, dtype=int)

#     f1_macro = f1_score(y_true, y_pred, average='macro')
#     f1_micro = f1_score(y_true, y_pred, average='micro')
#     precision_macro = precision_score(y_true, y_pred, average='macro')
#     recall_macro = recall_score(y_true, y_pred, average='macro')
#     auc_macro = roc_auc_score(y_true, all_pred_prob, average='macro')
#     pr_auc_macro = average_precision_score(y_true, all_pred_prob, average='macro')
#     hamming = hamming_loss(y_true, y_pred)

#     # æ¯ç±»æŒ‡æ ‡
#     f1_per_class = f1_score(y_true, y_pred, average=None)
#     precision_per_class = precision_score(y_true, y_pred, average=None)
#     recall_per_class = recall_score(y_true, y_pred, average=None)
#     auc_per_class = roc_auc_score(y_true, all_pred_prob, average=None)
#     support_per_class = y_true.sum(axis=0)

#     return auc_per_class, precision_per_class, recall_per_class, f1_per_class, support_per_class, all_pred_prob, f1_macro, f1_micro, precision_macro, recall_macro, auc_macro, pr_auc_macro, hamming

from sklearn.metrics import (
    f1_score, precision_score, recall_score, roc_auc_score,
    average_precision_score, hamming_loss
)
import torch.distributed as dist
import numpy as np
import torch
import math

def eval_node_classification_ddp_label(tgn, data, edge_idxs, batch_size, n_neighbors, num_classes, threshold=0.5):
    device = next(tgn.parameters()).device
    local_pred_prob = []
    local_true_labels = []

    num_instance = len(data.sources)
    num_batch = math.ceil(num_instance / batch_size)

    with torch.no_grad():
        tgn.eval()
        for k in range(num_batch):
            s_idx = k * batch_size
            e_idx = min(num_instance, s_idx + batch_size)

            sources_batch = data.sources[s_idx:e_idx]
            destinations_batch = data.destinations[s_idx:e_idx]
            timestamps_batch = data.timestamps[s_idx:e_idx]
            edge_idxs_batch = edge_idxs[s_idx:e_idx]
            labels_batch = data.labels[s_idx:e_idx]

            source_embedding, _, _ = tgn.module.compute_temporal_embeddings(
                sources_batch, destinations_batch, destinations_batch,
                timestamps_batch, edge_idxs_batch, n_neighbors
            )

            logits = tgn.module.node_classification_decoder(source_embedding)  # shape: [B, num_classes]
            probs = torch.sigmoid(logits).cpu().numpy()  # å¤šæ ‡ç­¾åˆ†ç±»ç”¨ sigmoid
            local_pred_prob.append(probs)
            local_true_labels.append(labels_batch)

    # æ‹¼æŽ¥æœ¬åœ° batch ç»“æžœ
    local_pred_prob = np.concatenate(local_pred_prob, axis=0)
    local_true_labels = np.concatenate(local_true_labels, axis=0)

    # DDP å¤šè¿›ç¨‹åŒæ­¥
    world_size = dist.get_world_size()
    local_pred_prob_tensor = torch.tensor(local_pred_prob, device=device)
    local_true_labels_tensor = torch.tensor(local_true_labels, device=device)

    gathered_preds = [torch.zeros_like(local_pred_prob_tensor) for _ in range(world_size)]
    gathered_trues = [torch.zeros_like(local_true_labels_tensor) for _ in range(world_size)]

    dist.all_gather(gathered_preds, local_pred_prob_tensor)
    dist.all_gather(gathered_trues, local_true_labels_tensor)

    all_pred_prob = torch.cat(gathered_preds, dim=0).cpu().numpy()
    all_true_labels = torch.cat(gathered_trues, dim=0).cpu().numpy()

    y_pred = (all_pred_prob >= threshold).astype(int)
    y_true = all_true_labels.astype(int)

    # è¯„ä¼°æŒ‡æ ‡
    try:
        auc_per_class = roc_auc_score(y_true, all_pred_prob, average=None)
        auc_macro = roc_auc_score(y_true, all_pred_prob, average='macro')
    except ValueError as e:
        print(f"[Warning] AUC error: {e}")
        auc_per_class = np.full(num_classes, np.nan)
        auc_macro = np.nan

    f1_per_class = f1_score(y_true, y_pred, average=None, zero_division=0)
    precision_per_class = precision_score(y_true, y_pred, average=None, zero_division=0)
    recall_per_class = recall_score(y_true, y_pred, average=None, zero_division=0)
    support_per_class = y_true.sum(axis=0)

    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)
    f1_micro = f1_score(y_true, y_pred, average='micro', zero_division=0)
    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)
    recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)
    pr_auc_macro = average_precision_score(y_true, all_pred_prob, average='macro')
    hamming = hamming_loss(y_true, y_pred)

    return auc_per_class, precision_per_class, recall_per_class, f1_per_class, support_per_class, all_pred_prob, f1_macro, f1_micro, precision_macro, recall_macro, auc_macro, pr_auc_macro, hamming
