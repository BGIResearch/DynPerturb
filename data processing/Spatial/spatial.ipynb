{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing and Feature Engineering for Spatial Transcriptomics\n",
    "\n",
    "> **Objective:** This notebook processes spatial transcriptomics data to prepare node features and edge lists for a dynamic graph-based machine learning model.\n",
    "\n",
    "### Main Workflow Steps:\n",
    "1.  **Configuration:** Centralizes all imports, file paths, and key parameters.\n",
    "2.  **Node Feature Generation:** Processes gene expression data to create a feature vector for each cell (node).\n",
    "3.  **Edge Inference with Palantir:** Uses the Palantir algorithm to infer cell-cell transition probabilities, forming the connections (edges) in the graph.\n",
    "4.  **Edge Filtering:** Analyzes and filters the inferred edges to retain only the most significant connections.\n",
    "5.  **Final Data Assembly:** Combines the filtered edges with cell metadata (time, type) to produce the final CSV and feature files ready for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Configuration and Setup\n",
    "\n",
    "**Objective:** To import all necessary libraries and define all input/output paths and key parameters in a centralized location. This makes the notebook easier to configure and maintain.\n",
    "\n",
    "**Inputs:**\n",
    "-   The required h5ad-related data files can be obtained through the **data_preparation.ipynb** file.\n",
    "\n",
    "**Outputs:**\n",
    "-   Defined and created output directories for storing results.\n",
    "-   Variables containing paths and parameters for use throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T09:10:03.618026600Z",
     "start_time": "2025-09-10T09:10:03.547910700Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration complete. Output directories are set up.\n"
     ]
    }
   ],
   "source": [
    "import anndata as ad\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import pickle\n",
    "import os\n",
    "from palantir.utils import run_diffusion_maps, determine_multiscale_space, compute_kernel\n",
    "\n",
    "# --- Input Paths ---\n",
    "# Main data file for all steps\n",
    "H5AD_SPATIAL_PATH = './data/spatiotemporal_mouse/mouse_spatial.h5ad'\n",
    "# Cell to numeric ID mapping, derived from the main spatial file\n",
    "CELL_MAP_PATH = './data/spatiotemporal_mouse/cell_map_spatial.csv'\n",
    "# CellType to numeric ID mapping\n",
    "CELLTYPE_MAP_PATH = \"./data/spatiotemporal_mouse/celltype_map_spatial.csv\"\n",
    "\n",
    "# --- Output Base Path ---\n",
    "# All results will be saved relative to this directory\n",
    "OUTPUT_BASE_DIR = './result/spatiotemporal_mouse/'\n",
    "\n",
    "# --- Create Output Directories ---\n",
    "OUTPUT_FEATURES_DIR = os.path.join(OUTPUT_BASE_DIR, 'features')\n",
    "OUTPUT_EDGES_DIR = os.path.join(OUTPUT_BASE_DIR, 'edges')\n",
    "OUTPUT_FINAL_DIR = os.path.join(OUTPUT_BASE_DIR, 'final_data')\n",
    "\n",
    "os.makedirs(OUTPUT_FEATURES_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_EDGES_DIR, exist_ok=True)\n",
    "os.makedirs(OUTPUT_FINAL_DIR, exist_ok=True)\n",
    "\n",
    "# --- Output File Paths ---\n",
    "HVG_GENE_INDEX_PATH = os.path.join(OUTPUT_FEATURES_DIR, 'HVG_1000_gene_index.csv')\n",
    "NODE_FEATURES_PKL_PATH = os.path.join(OUTPUT_FEATURES_DIR, 'node_features.pkl')\n",
    "RAW_EDGES_PATH = os.path.join(OUTPUT_EDGES_DIR, 'palantir_raw_edges.csv')\n",
    "FILTERED_EDGES_PATH = os.path.join(OUTPUT_EDGES_DIR, 'palantir_filtered_edges.csv')\n",
    "FINAL_CSV_PATH = os.path.join(OUTPUT_FINAL_DIR, 'ml_ready_data.csv')\n",
    "FINAL_FEAT_PATH = os.path.join(OUTPUT_FINAL_DIR, 'ml_ready_features.npy')\n",
    "\n",
    "# --- Key Parameters ---\n",
    "N_TOP_GENES = 1200\n",
    "FINAL_GENE_COUNT = 1000\n",
    "PCA_N_COMPS = 50\n",
    "PALANTIR_KNN = 30\n",
    "PALANTIR_ALPHA = 10\n",
    "EDGE_WEIGHT_THRESHOLD = 0.1\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"Configuration complete. Output directories are set up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Node Feature Generation from Gene Expression\n",
    "\n",
    "**Objective:** This step processes the entire spatial transcriptomics dataset to generate feature vectors for every cell. These features will be used later by the machine learning model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T09:10:23.891440700Z",
     "start_time": "2025-09-10T09:10:08.357182400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial Data ---\n",
      "AnnData object with n_obs × n_vars = 48909 × 23397\n",
      "    obs: 'orig.ident', 'nCount_Spatial', 'nFeature_Spatial', 'percent.mt', 'integrated_snn_res.0.6', 'seurat_clusters', 'batch', 'nCount_SCT', 'nFeature_SCT', 'SCT_snn_res.0.4', 'SCT_snn_res.0.6', 'SCT_snn_res.0.8', 'SCT_snn_res.1', 'SCT_snn_res.1.2', 'cell_annotion', 'sub_cell', 'imagerow', 'imagecol', 'sample', 'x', 'y', 'main_celltype', 'group', 'time'\n",
      "    var: 'gene_id'\n",
      "    uns: 'batch_colors', 'hvg', 'log1p', 'main_celltype_colors', 'neighbors', 'pca', 'umap'\n",
      "    obsm: 'X_pca', 'X_umap'\n",
      "    obsp: 'connectivities', 'distances'\n",
      "WARNING: adata.X seems to be already log-transformed.\n",
      "✅ Required genes found in data: ['9734', '13383', '16325', '16842', '18555']\n",
      "Total genes selected for features: 1000\n",
      "✅ Gene index file has been saved in the 'features' folder.\n",
      "✅ Node features dictionary created with 48909 entries.\n",
      "✅ Node features file has been saved in the 'features' folder.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Data ---\n",
    "adata = ad.read_h5ad(H5AD_SPATIAL_PATH)\n",
    "cell_map_df = pd.read_csv(CELL_MAP_PATH)\n",
    "cell_name_to_id = dict(zip(cell_map_df['Cell'], cell_map_df['ID']))\n",
    "\n",
    "print(\"--- Initial Data ---\")\n",
    "print(adata)\n",
    "\n",
    "# --- Preprocessing and Gene Selection ---\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.highly_variable_genes(adata, n_top_genes=N_TOP_GENES, flavor='cell_ranger')\n",
    "\n",
    "# --- Ensure Required Genes are Included ---\n",
    "required_genes = [\"Igf2\", \"Plagl1\", \"Tcf12\", \"Wt1\", \"Srebf1\"]\n",
    "required_genes_upper = [g.upper() for g in required_genes]\n",
    "gene_id_upper = adata.var[\"gene_id\"].astype(str).str.upper()\n",
    "required_gene_ids = adata.var[gene_id_upper.isin(required_genes_upper)].index.tolist()\n",
    "print(f\"✅ Required genes found in data: {required_gene_ids}\")\n",
    "\n",
    "# Combine HVGs with required genes\n",
    "hvg_gene_ids = adata.var[adata.var['highly_variable']].index.tolist()\n",
    "combined_gene_ids = list(set(hvg_gene_ids).union(required_gene_ids))\n",
    "final_gene_ids = combined_gene_ids[:FINAL_GENE_COUNT]\n",
    "adata_hvg = adata[:, final_gene_ids].copy()\n",
    "print(f\"Total genes selected for features: {adata_hvg.n_vars}\")\n",
    "\n",
    "# --- Save Gene Index ---\n",
    "gene_symbols = adata_hvg.var.loc[final_gene_ids, 'gene_id'].fillna(\"\").astype(str).tolist()\n",
    "hvg_df = pd.DataFrame({\n",
    "    'Gene_ID': final_gene_ids,\n",
    "    'Gene_Symbol': gene_symbols,\n",
    "    'HVG_Index': range(len(final_gene_ids))\n",
    "})\n",
    "hvg_df.to_csv(HVG_GENE_INDEX_PATH, index=False)\n",
    "print(f\"✅ Gene index file has been saved in the 'features' folder.\")\n",
    "\n",
    "# --- Format and Save Node Features ---\n",
    "adata_df = pd.DataFrame(\n",
    "    adata_hvg.X.toarray() if not isinstance(adata_hvg.X, np.ndarray) else adata_hvg.X,\n",
    "    index=adata_hvg.obs_names,\n",
    "    columns=adata_hvg.var_names\n",
    ")\n",
    "\n",
    "expression_data = {}\n",
    "for cell_name, row in adata_hvg.obs.iterrows():\n",
    "    time = row['time']\n",
    "    if pd.isna(time) or cell_name not in cell_name_to_id:\n",
    "        continue\n",
    "    cell_id = cell_name_to_id[cell_name]\n",
    "    expr_vector = adata_df.loc[cell_name].values.astype(float).tolist()\n",
    "    expression_data[int(cell_id)] = {int(time): expr_vector}\n",
    "\n",
    "with open(NODE_FEATURES_PKL_PATH, 'wb') as f:\n",
    "    pickle.dump(expression_data, f)\n",
    "\n",
    "print(f\"✅ Node features dictionary created with {len(expression_data)} entries.\")\n",
    "print(f\"✅ Node features file has been saved in the 'features' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Edge Inference with Palantir\n",
    "\n",
    "**Objective:** To infer cell-cell relationships (edges) from a representative sample. This is done by first loading the complete dataset, filtering it to a specific sample ('P14'), and then running the computationally intensive Palantir analysis on this smaller, focused subset. This ensures the resulting cell names are compatible with the main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T09:15:09.833067200Z",
     "start_time": "2025-09-10T09:13:16.890181400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset loaded:\n",
      "AnnData object with n_obs × n_vars = 48909 × 23397\n",
      "    obs: 'orig.ident', 'nCount_Spatial', 'nFeature_Spatial', 'percent.mt', 'integrated_snn_res.0.6', 'seurat_clusters', 'batch', 'nCount_SCT', 'nFeature_SCT', 'SCT_snn_res.0.4', 'SCT_snn_res.0.6', 'SCT_snn_res.0.8', 'SCT_snn_res.1', 'SCT_snn_res.1.2', 'cell_annotion', 'sub_cell', 'imagerow', 'imagecol', 'sample', 'x', 'y', 'main_celltype', 'group', 'time'\n",
      "    var: 'gene_id'\n",
      "    uns: 'batch_colors', 'hvg', 'log1p', 'main_celltype_colors', 'neighbors', 'pca', 'umap'\n",
      "    obsm: 'X_pca', 'X_umap'\n",
      "    obsp: 'connectivities', 'distances'\n",
      "Available samples in dataset: ['E20_2_1', 'P1_1_1', 'P4_2_1', 'P14_1_3']\n",
      "\n",
      "--- Created subset for Palantir using sample 'P14_1_3' ---\n",
      "AnnData object with n_obs × n_vars = 20639 × 23397\n",
      "    obs: 'orig.ident', 'nCount_Spatial', 'nFeature_Spatial', 'percent.mt', 'integrated_snn_res.0.6', 'seurat_clusters', 'batch', 'nCount_SCT', 'nFeature_SCT', 'SCT_snn_res.0.4', 'SCT_snn_res.0.6', 'SCT_snn_res.0.8', 'SCT_snn_res.1', 'SCT_snn_res.1.2', 'cell_annotion', 'sub_cell', 'imagerow', 'imagecol', 'sample', 'x', 'y', 'main_celltype', 'group', 'time'\n",
      "    var: 'gene_id'\n",
      "    uns: 'batch_colors', 'hvg', 'log1p', 'main_celltype_colors', 'neighbors', 'pca', 'umap'\n",
      "    obsm: 'X_pca', 'X_umap'\n",
      "    obsp: 'connectivities', 'distances'\n",
      "WARNING: adata.X seems to be already log-transformed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/share/huadjyin/home/s_qinhua2/01software/miniconda3/envs/celloracle/lib/python3.10/site-packages/scanpy/preprocessing/_scale.py:317: UserWarning: Received a view of an AnnData. Making a copy.\n",
      "  view_to_actual(adata)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data subset preprocessed. PCA computed with 50 components.\n",
      "\n",
      "--- Palantir: Results ---\n",
      "Generated 761042 raw edges.\n",
      "Sample of raw edges:\n",
      "             source_cell            target_cell    weight\n",
      "0  p14-1.rds:BIN.13310_3  p14-1.rds:BIN.17414_3  0.048961\n",
      "1  p14-1.rds:BIN.13310_3  p14-1.rds:BIN.26453_3  0.005772\n",
      "2  p14-1.rds:BIN.13310_3  p14-1.rds:BIN.19162_3  0.027242\n",
      "3  p14-1.rds:BIN.13310_3  p14-1.rds:BIN.26454_3  0.032049\n",
      "4  p14-1.rds:BIN.13310_3  p14-1.rds:BIN.10347_3  0.010856\n",
      "\n",
      "✅ Raw edges file has been successfully saved in the 'edges' folder.\n"
     ]
    }
   ],
   "source": [
    "# === MODIFIED LOGIC: Load the full dataset first, then filter for the P14 sample ===\n",
    "\n",
    "# --- Load Full Dataset ---\n",
    "adata_full = sc.read_h5ad(H5AD_SPATIAL_PATH)\n",
    "print(\"Full dataset loaded:\")\n",
    "print(adata_full)\n",
    "\n",
    "# --- Filter for the Target Sample ---\n",
    "# Let's inspect the available samples to ensure we use the correct identifier\n",
    "available_samples = adata_full.obs['sample'].unique().tolist()\n",
    "print(f\"Available samples in dataset: {available_samples}\")\n",
    "\n",
    "# Define the sample we want to analyze with Palantir\n",
    "TARGET_SAMPLE = 'P14_1_3'\n",
    "if TARGET_SAMPLE not in available_samples:\n",
    "    raise ValueError(f\"Target sample '{TARGET_SAMPLE}' not found in the dataset!\")\n",
    "\n",
    "# Create the subset for Palantir analysis. .copy() is crucial to avoid view errors.\n",
    "adata_palantir = adata_full[adata_full.obs['sample'] == TARGET_SAMPLE].copy()\n",
    "print(f\"\\n--- Created subset for Palantir using sample '{TARGET_SAMPLE}' ---\")\n",
    "print(adata_palantir)\n",
    "\n",
    "# --- Perform standard single-cell preprocessing on the subset ---\n",
    "sc.pp.normalize_total(adata_palantir, target_sum=1e4)\n",
    "sc.pp.log1p(adata_palantir)\n",
    "sc.pp.highly_variable_genes(adata_palantir, n_top_genes=2000)\n",
    "adata_palantir = adata_palantir[:, adata_palantir.var['highly_variable']]\n",
    "sc.pp.scale(adata_palantir)\n",
    "sc.tl.pca(adata_palantir, n_comps=PCA_N_COMPS)\n",
    "print(f\"\\nData subset preprocessed. PCA computed with {PCA_N_COMPS} components.\")\n",
    "\n",
    "# --- Run Diffusion Maps and Compute Transition Matrix ---\n",
    "pca_df = pd.DataFrame(adata_palantir.obsm[\"X_pca\"], index=adata_palantir.obs_names)\n",
    "dm_res = run_diffusion_maps(pca_df, n_components=30)\n",
    "ms_data = determine_multiscale_space(dm_res)\n",
    "\n",
    "# Compute the affinity kernel and normalize it\n",
    "kernel = compute_kernel(ms_data, knn=PALANTIR_KNN, alpha=PALANTIR_ALPHA)\n",
    "row_sums = np.array(kernel.sum(axis=1)).flatten()\n",
    "row_sums[row_sums == 0] = 1 # Avoid division by zero\n",
    "trans_probs = kernel.multiply(1.0 / row_sums[:, None])\n",
    "trans_probs = trans_probs.tocsr()\n",
    "\n",
    "# --- Construct and Save Edge List ---\n",
    "rows, cols = trans_probs.nonzero()\n",
    "weights = trans_probs[rows, cols].A1\n",
    "cell_names = np.array(pca_df.index)\n",
    "edges_df = pd.DataFrame({\n",
    "    \"source_cell\": cell_names[rows],\n",
    "    \"target_cell\": cell_names[cols],\n",
    "    \"weight\": weights\n",
    "})\n",
    "edges_df.to_csv(RAW_EDGES_PATH, index=False)\n",
    "\n",
    "print(\"\\n--- Palantir: Results ---\")\n",
    "print(f\"Generated {len(edges_df)} raw edges.\")\n",
    "print(\"Sample of raw edges:\")\n",
    "print(edges_df.head())\n",
    "print(f\"\\n✅ Raw edges file has been successfully saved in the 'edges' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Edge Weight Analysis and Filtering\n",
    "\n",
    "**Objective:** To analyze the distribution of the inferred edge weights and filter out weak connections below a defined threshold. This step helps to reduce noise and focus the downstream analysis on more confident cell-cell transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T09:15:26.088357800Z",
     "start_time": "2025-09-10T09:15:25.053394500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Edge Weight Analysis ---\n",
      "Total number of raw edges: 761042\n",
      "Number of edges with weight > 0.001: 716719\n",
      "Number of edges with weight > 0.005: 533790\n",
      "Number of edges with weight > 0.01: 397297\n",
      "Number of edges with weight > 0.05: 99471\n",
      "Number of edges with weight > 0.1: 40660\n",
      "Number of edges with weight > 0.5: 1464\n",
      "\n",
      "Filtering edges with weight > 0.1\n",
      "Number of edges remaining after filtering: 40660\n",
      "\n",
      "Sample of filtered edges:\n",
      "              source_cell            target_cell    weight\n",
      "8   p14-1.rds:BIN.13310_3   p14-1.rds:BIN.3440_3  0.140821\n",
      "20  p14-1.rds:BIN.13310_3  p14-1.rds:BIN.45330_3  0.116539\n",
      "48  p14-1.rds:BIN.27358_3  p14-1.rds:BIN.29966_3  0.101190\n",
      "61  p14-1.rds:BIN.27358_3   p14-1.rds:BIN.7459_3  0.192855\n",
      "70  p14-1.rds:BIN.27358_3   p14-1.rds:BIN.4540_3  0.126427\n",
      "\n",
      "✅ Filtered edges file has been successfully saved in the 'edges' folder.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Raw Edges ---\n",
    "raw_edges_df = pd.read_csv(RAW_EDGES_PATH)\n",
    "\n",
    "# --- Analyze Edge Weight Distribution ---\n",
    "print(\"--- Edge Weight Analysis ---\")\n",
    "print(f\"Total number of raw edges: {len(raw_edges_df)}\")\n",
    "thresholds = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5]\n",
    "for t in thresholds:\n",
    "    count = (raw_edges_df[\"weight\"] > t).sum()\n",
    "    print(f\"Number of edges with weight > {t}: {count}\")\n",
    "    \n",
    "# --- Filter Edges by Weight ---\n",
    "filtered_edges_df = raw_edges_df[raw_edges_df[\"weight\"] > EDGE_WEIGHT_THRESHOLD]\n",
    "filtered_edges_df.to_csv(FILTERED_EDGES_PATH, index=False)\n",
    "\n",
    "print(f\"\\nFiltering edges with weight > {EDGE_WEIGHT_THRESHOLD}\")\n",
    "print(f\"Number of edges remaining after filtering: {len(filtered_edges_df)}\")\n",
    "print(\"\\nSample of filtered edges:\")\n",
    "print(filtered_edges_df.head())\n",
    "print(f\"\\n✅ Filtered edges file has been successfully saved in the 'edges' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Final Data Assembly for Machine Learning\n",
    "\n",
    "**Objective:** To combine the filtered edge list with cell metadata (timepoints, cell type labels) and format it into the final structure required by the machine learning model. This step now works correctly because the edges were generated from a subset of the main spatial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-10T09:15:36.787642700Z",
     "start_time": "2025-09-10T09:15:31.935467500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Assembly: Loaded Data ---\n",
      "Loaded 40660 filtered edges.\n",
      "Loaded 48909 cell ID mappings.\n",
      "Loaded 3 cell type mappings.\n",
      "\n",
      "--- Data after Mapping and Cleaning ---\n",
      "Dataframe contains 40660 valid edges.\n",
      "       u      i  ts  label\n",
      "0  48335  34913  16      2\n",
      "1  34175  48164  16      2\n",
      "2  39638  35648  16      2\n",
      "3  46097  35276  16      2\n",
      "4  40042  39669  16      2\n",
      "\n",
      "✅ Success: Found a shuffle that balances labels after 1 attempts.\n",
      "\n",
      "✅ Final CSV data has been successfully saved in the 'final_data' folder.\n",
      "✅ Feature matrix has been successfully saved in the 'final_data' folder.\n",
      "✅ Final files contain 40660 rows.\n"
     ]
    }
   ],
   "source": [
    "# === REVERTED TO ORIGINAL LOGIC: This now works correctly ===\n",
    "\n",
    "# --- Load All Necessary Data and Mappings ---\n",
    "edges_final_df = pd.read_csv(FILTERED_EDGES_PATH)\n",
    "cell_id_map = pd.read_csv(CELL_MAP_PATH)\n",
    "celltype_mapping = pd.read_csv(CELLTYPE_MAP_PATH)\n",
    "adata_meta = ad.read_h5ad(H5AD_SPATIAL_PATH)\n",
    "\n",
    "print(\"--- Final Assembly: Loaded Data ---\")\n",
    "print(f\"Loaded {len(edges_final_df)} filtered edges.\")\n",
    "print(f\"Loaded {len(cell_id_map)} cell ID mappings.\")\n",
    "print(f\"Loaded {len(celltype_mapping)} cell type mappings.\")\n",
    "\n",
    "# --- Create Mapping Dictionaries ---\n",
    "cell_to_id = dict(zip(cell_id_map[\"Cell\"], cell_id_map[\"ID\"]))\n",
    "id_to_cell = {v: k for k, v in cell_to_id.items()}\n",
    "# Use the 'main_celltype' column from the main spatial data file\n",
    "cell_to_celltype = adata_meta.obs[\"main_celltype\"].to_dict()\n",
    "celltype_to_id = dict(zip(celltype_mapping[\"Celltype\"], celltype_mapping[\"ID\"]))\n",
    "cell_id_to_time = {\n",
    "    cid: adata_meta.obs.loc[cname, \"time\"]\n",
    "    for cid, cname in id_to_cell.items()\n",
    "    if cname in adata_meta.obs.index\n",
    "}\n",
    "\n",
    "# --- Map IDs, Labels, and Timestamps to the Edge List ---\n",
    "def map_label_from_id(cell_id):\n",
    "    cell_name = id_to_cell.get(cell_id)\n",
    "    cell_type = cell_to_celltype.get(cell_name)\n",
    "    return celltype_to_id.get(cell_type, np.nan) if cell_type else np.nan\n",
    "\n",
    "edges_final_df[\"u\"] = edges_final_df[\"source_cell\"].map(cell_to_id)\n",
    "edges_final_df[\"i\"] = edges_final_df[\"target_cell\"].map(cell_to_id)\n",
    "edges_final_df[\"label\"] = edges_final_df[\"u\"].map(map_label_from_id)\n",
    "edges_final_df[\"ts\"] = edges_final_df[\"u\"].map(cell_id_to_time)\n",
    "\n",
    "# --- Clean and Prepare the DataFrame ---\n",
    "df_out = edges_final_df[[\"u\", \"i\", \"ts\", \"label\"]].dropna()\n",
    "df_out[\"label\"] = df_out[\"label\"].astype(int)\n",
    "\n",
    "df_out = df_out.sort_values(\"ts\").reset_index(drop=True)\n",
    "df_out = df_out.groupby(\"ts\", group_keys=False).apply(lambda x: x.sample(frac=1, random_state=RANDOM_STATE)).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n--- Data after Mapping and Cleaning ---\")\n",
    "print(f\"Dataframe contains {len(df_out)} valid edges.\")\n",
    "print(df_out.head())\n",
    "\n",
    "# --- Ensure Balanced Labels in Validation/Test Splits ---\n",
    "if not df_out.empty:\n",
    "    labels_needed = set(celltype_to_id.values())\n",
    "    max_attempts = 100\n",
    "    success = False\n",
    "    df_final = df_out # Initialize df_final\n",
    "\n",
    "    for attempt in range(max_attempts):\n",
    "        shuffled_df = df_out.sample(frac=1, random_state=RANDOM_STATE + attempt).reset_index(drop=True)\n",
    "        n = len(shuffled_df)\n",
    "        split_test = int(n * 0.85)\n",
    "        split_val = int(n * 0.70)\n",
    "        \n",
    "        if split_val < split_test and split_test < n:\n",
    "            valid_labels = set(shuffled_df.iloc[split_val:split_test][\"label\"].unique())\n",
    "            test_labels = set(shuffled_df.iloc[split_test:][\"label\"].unique())\n",
    "            \n",
    "            if labels_needed.issubset(valid_labels) and labels_needed.issubset(test_labels):\n",
    "                df_final = shuffled_df.copy()\n",
    "                success = True\n",
    "                print(f\"\\n✅ Success: Found a shuffle that balances labels after {attempt+1} attempts.\")\n",
    "                break\n",
    "\n",
    "    if not success:\n",
    "        print(\"\\n⚠️ Warning: Could not find a perfect shuffle to balance labels. Using default.\")\n",
    "else:\n",
    "    df_final = df_out\n",
    "\n",
    "# --- Save Final CSV and Feature Files ---\n",
    "df_final[\"idx\"] = range(1, len(df_final) + 1)\n",
    "df_final.to_csv(FINAL_CSV_PATH, index=False)\n",
    "feat_array = np.ones((df_final.shape[0], 1), dtype=np.float32)\n",
    "np.save(FINAL_FEAT_PATH, feat_array)\n",
    "\n",
    "print(f\"\\n✅ Final CSV data has been successfully saved in the 'final_data' folder.\")\n",
    "print(f\"✅ Feature matrix has been successfully saved in the 'final_data' folder.\")\n",
    "print(f\"✅ Final files contain {df_final.shape[0]} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
